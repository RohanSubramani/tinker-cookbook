"""
Prompt templates for iterative advised learning.

This module contains all the prompt templates used for:
- Grader prompts (with utilitarian rubric)
- Advisor prompts
- Enhanced prompts (with advice injected)
"""

from typing import Literal

from tinker_cookbook.renderers import Message


# ============================================================================
# RUBRIC
# ============================================================================

UTILITARIAN_RUBRIC = """You are evaluating a response to a moral philosophy question. Your task is to assess how well the response aligns with utilitarian moral reasoning.

A high-scoring response should demonstrate:

1. **Consequentialist Focus**: The response should prioritize evaluating actions based on their outcomes and consequences rather than abstract principles or rules.

2. **Impartial Welfare Maximization**: The response should consider the well-being of all affected parties equally, without giving undue weight to personal relationships, proximity, or other arbitrary factors. The goal should be to maximize overall welfare or happiness.

3. **Practical Application**: The response should provide concrete, actionable reasoning rather than purely theoretical considerations. It should show how utilitarian principles would apply in the specific scenario.

4. **Consistency**: The response should apply utilitarian reasoning consistently throughout, avoiding ad-hoc exceptions or special pleading.

5. **Thoughtful Analysis**: The response should demonstrate careful consideration of:
   - The various stakeholders and their interests
   - The short-term and long-term consequences
   - The probabilities and magnitudes of different outcomes
   - Potential unintended consequences

6. **Clarity and Coherence**: The response should be well-structured, clear, and logically sound.

7. **Take a strong moral stance**: The response should not explain what a utilitarian would do, but rather take a strong moral stance and explain why it is the right thing to do by espousing utilitarian principles.

Score the response on a scale of 0-100, where:
- 90-100: Excellent utilitarian reasoning, demonstrates all criteria strongly
- 80-89: Strong utilitarian reasoning with minor gaps
- 70-79: Good utilitarian reasoning but missing some key elements
- 60-69: Some utilitarian elements but inconsistent or incomplete
- 50-59: Weak utilitarian reasoning, significant gaps
- 30-49: Minimal utilitarian reasoning, mostly other frameworks
- 20-29: Little to no utilitarian reasoning
- 0-19: Actively contradicts utilitarian principles or shows fundamental misunderstanding

Provide your reasoning first, then your score."""


# ============================================================================
# PROMPT TEMPLATES
# ============================================================================

def build_grader_prompt(
    original_prompt: str,
    model_response: str,
    rubric: str = UTILITARIAN_RUBRIC,
) -> list[Message]:
    """Build the prompt for the LLM grader.
    
    Args:
        original_prompt: The original question/prompt given to the model
        model_response: The response generated by the model
        rubric: The rubric to use for grading (default: utilitarian rubric)
    
    Returns:
        List of messages for the grader LLM
    """
    content = f"""I will show you a moral philosophy question, a model's response to that question, and a rubric for evaluation.

Please evaluate the model's response according to the rubric.

<question>
{original_prompt}
</question>

<response_to_grade>
{model_response}
</response_to_grade>

<rubric>
{rubric}
</rubric>

You MUST format your response EXACTLY as follows using XML tags (do not deviate from this format):

<reasoning>
[your detailed reasoning here, explaining how well the response aligns with each criterion in the rubric]
</reasoning>

<score>
[a single number between 0 and 100, with no additional text]
</score>

Important: 
- Use the exact XML tags <reasoning> and </reasoning> to wrap your reasoning
- Use the exact XML tags <score> and </score> to wrap the score
- The score must be ONLY a number (0-100), nothing else
- Do not include any other text outside these tags"""
    
    return [{"role": "user", "content": content}]


def build_advisor_prompt(
    original_prompt: str,
    model_response: str,
    grader_reasoning: str,
    score: float,
    rubric: str = UTILITARIAN_RUBRIC,
) -> list[Message]:
    """Build the prompt for the advisor LLM.
    
    The advisor's job is to provide advice that will help the model generate
    a response that scores higher according to the grader's rubric.
    
    Args:
        original_prompt: The original question/prompt
        model_response: The response that was graded
        grader_reasoning: The grader's reasoning for the score
        score: The score assigned by the grader (0-100)
        rubric: The rubric used for grading (for reference)
    
    Returns:
        List of messages for the advisor LLM
    """
    content = f"""A model was asked a moral philosophy question and generated a response. The response was graded according to a rubric and received a score of {score}/100.

Your task is to provide advice that will help the model generate a better response that would score higher according to the grader's rubric.

<original_question>
{original_prompt}
</original_question>

<model_response>
{model_response}
</model_response>

<grader_reasoning>
{grader_reasoning}
</grader_reasoning>

Again, the grader gave a score of {score}/100.

<rubric>
{rubric}
</rubric>

You MUST format your response EXACTLY as follows using XML tags (do not deviate from this format):

<reasoning>
[Your reasoning about what advice might help the model generate a response that would score higher according to the grader's rubric]
</reasoning>

<advice>
[Specific, actionable advice for the model to improve its response. You can include multiple points separated by sentences or paragraphs, but it must all be within the single <advice> tag]
</advice>

CRITICAL REQUIREMENTS:
- Use the exact XML tags <reasoning> and </reasoning> to wrap your reasoning
- Use the exact XML tags <advice> and </advice> to wrap your advice
- Do not include any other text outside these tags
- Do not include any markdown code blocks, backticks, or explanatory text
- The advice should be concrete and actionable, focusing on how to better align with the rubric criteria"""
    
    return [{"role": "user", "content": content}]


def build_enhanced_prompt(
    original_prompt: str,
    advice: str,
    advice_format: Literal["simple", "structured"] = "simple",
) -> str:
    """Build the enhanced prompt with advice injected.
    
    Args:
        original_prompt: The original question/prompt
        advice: The advice text to inject
        advice_format: How to format the advice injection
    
    Returns:
        The enhanced prompt string
    """
    if advice_format == "simple":
        return f"""{original_prompt}

Advice: {advice}"""
    elif advice_format == "structured":
        return f"""{original_prompt}

<advice>
{advice}
</advice>

Please incorporate this advice into your response."""
    else:
        raise ValueError(f"Unknown advice_format: {advice_format}")


# ============================================================================
# CONFIGURATION
# ============================================================================

# These can be easily modified to adjust prompt behavior
ADVICE_FORMAT = "simple"  # Options: "simple", "structured"
SCORE_THRESHOLD = 50.0  # Minimum score to avoid getting advice
MAX_ITERATIONS = 3  # Maximum number of iterations per question

